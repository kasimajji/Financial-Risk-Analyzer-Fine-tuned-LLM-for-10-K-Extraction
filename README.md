# ğŸ“Š FinLLM: Fine-Tuned LLM for Risk Extraction from 10-K Filings (QLoRA)

Extracting structured **risk insights** from SEC 10-K filings using a fine-tuned LLaMA-2/Mistral model â€” built for **precision**, **explainability**, and **real-world financial NLP**.

> This is not just another GenAI demo â€” it's a domain-adapted LLM that understands financial filings and produces **auditable JSON risk summaries**.

---

## ğŸ§  Project Architecture & Flow

<p align="center">
  <img src="./assets/architecture_pipeline.png" width="680"/>
</p>

| Step                     | Description                                     |
| ------------------------ | ----------------------------------------------- |
| 1ï¸âƒ£ Data Ingestion     | Parse and chunk 10-K risk and MD&A sections     |
| 2ï¸âƒ£ Label Formatting   | JSON schema for Risk Type, Severity, Mitigation |
| 3ï¸âƒ£ Tokenizer Prep     | Align SentencePiece tokenizer with Mistral      |
| 4ï¸âƒ£ QLoRA Finetuning   | Train lightweight LoRA adapters on ~4k samples  |
| 5ï¸âƒ£ Inference Pipeline | Generate structured JSON from natural prompts   |

---

## ğŸ“ Folder Structure

```
finllm/
â”œâ”€â”€ data/              # Raw & preprocessed 10-K filings
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ qlora_adapters/   # LoRA fine-tuned adapter weights
â”‚   â””â”€â”€ tokenizer/        # Aligned tokenizer assets
â”œâ”€â”€ inference/         # Inference scripts
â”œâ”€â”€ evaluation/        # Rouge, hallucination scores
â”œâ”€â”€ app/               # Streamlit frontend (optional)
â”œâ”€â”€ prompts/           # Prompt templates
â”œâ”€â”€ taxonomy/          # Risk taxonomy schema
â”œâ”€â”€ offload/           # Disk offloading dir for large models
```

---

## ğŸ” Step-by-Step Execution

### âœ… 1. Data Preprocessing

- Extracted **Risk Factor** and **MD&A** sections from 10-Ks
- Cleaned HTML/tables, normalized spacing
- Converted to HF `Dataset` for tokenization

### ğŸ· 2. Schema-Based Labeling

- Designed a structured label format with: `description`, `type`, `severity`, `probability`, `impact`, `mitigation`

```json
{
  "risk": {
    "description": "...",
    "type": "cybersecurity",
    "severity": "high",
    "probability": "medium",
    "impact": "high",
    "mitigation": "..."
  }
}
```

### ğŸ§ª 3. Finetuning with QLoRA

- Model: `mistralai/Mistral-7B-v0.1`
- Training method: QLoRA (4-bit quantized)
- Adapter saved to: `model/qlora_adapters`
- Trained on labeled prompts using HuggingFace Trainer

---

## âš™ï¸ Why QLoRA?

- ğŸš€ **Low Memory Footprint**: 4-bit quantization supports 7B models on a single A100
- ğŸ’¸ **Cost-Efficient**: Only adapter weights trained/saved (~300MB)
- ğŸ”„ **Reusable**: LoRA adapters can be swapped onto newer models easily
- ğŸ“¦ **Compatible**: Integrates seamlessly with HuggingFace `Trainer` and PEFT

---

## ğŸš€ Inference Demo (Real Results)

We tested two different 10-K prompts through our finetuned model.

### ğŸ“‚ Example 1: Financial Risk

<table>
<thead>
<tr>
<th align="center">ğŸ“¥ Input</th>
<th align="center">ğŸ“¤ Generated JSON Output</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><img src="./assets/input_financial.png" width="420"/></td>
<td align="center"><img src="./assets/output_financial.png" width="420"/></td>
</tr>
</tbody>
</table>

---

### ğŸ” Example 2: Cybersecurity Risk

<table>
<thead>
<tr>
<th align="center">ğŸ“¥ Input</th>
<th align="center">ğŸ“¤ Generated JSON Output</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><img src="./assets/input_cybersecurity.png" width="420"/></td>
<td align="center"><img src="./assets/output_cybersecurity.png" width="420"/></td>
</tr>
</tbody>
</table>

> ğŸ“Œ These outputs prove our model **learned to identify and classify risks** â€” and format them into actionable, structured data.

## ğŸ¯ What We Achieved

âœ” Fine-tuned Mistral using QLoRA on real-world SEC data
âœ” Created a JSON-based labeling schema for risk extraction
âœ” Designed an inference pipeline with `generate()` + custom prompts
âœ” Generated **auditable risk summaries** from natural 10-K sections
âœ” Demonstrated model generalization to **cybersecurity, financial, and operational risks**

---

## ğŸ§° Tech Stack

- ğŸ§  Mistral 7B + QLoRA via HuggingFace PEFT
- ğŸ¤— Transformers, Datasets, accelerate, bitsandbytes
- ğŸ“¦ SentencePiece tokenizer
- ğŸ”— Google Colab (LoRA training & inference tested)

---

## ğŸ™‹â€â™‚ï¸ About the Author

Built with care by [KasiMajji](https://www.linkedin.com/in/kasi-majji/) â€” an AI Engineer passionate about building **explainable, real-world GenAI systems**.

> Letâ€™s connect â€” especially if you care about **LLMs, RAGs, and AI that actually works.**
